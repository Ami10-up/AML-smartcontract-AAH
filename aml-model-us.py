# -*- coding: utf-8 -*-
"""Copy of AML1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iOz8864bZobkhEGFUl5lkHyedNkRXvoB
"""

from google.colab import auth
auth.authenticate_user()

# The corrected SQL query with 'from_address' in the GROUP BY clause
sql_query = """
SELECT
    from_address,
    COUNT(*) AS total_transactions_sent,
    SUM(CAST(value AS BIGNUMERIC) / 1e18) AS total_ether_sent,
    AVG(CAST(value AS BIGNUMERIC) / 1e18) AS avg_ether_sent,
    COUNT(DISTINCT to_address) AS unique_addresses_sent_to,
    MIN(block_timestamp) AS first_transaction_timestamp,
    MAX(block_timestamp) AS last_transaction_timestamp,
    AVG(gas_price) AS avg_gas_price
FROM
    `bigquery-public-data.crypto_ethereum.transactions`
WHERE
    block_timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
GROUP BY
    from_address  -- <-- This line fixes the error
ORDER BY
    total_transactions_sent DESC
LIMIT
    100000;
"""

from google.cloud import bigquery
import pandas as pd

project_id = 'aml-project-wf'
client = bigquery.Client(project=project_id)

df = client.query(sql_query).to_dataframe()
print("Query complete!")

# CELL 4: FEATURE ENGINEERING
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create a feature for the account's active duration in days
df['account_active_days'] = (df['last_transaction_timestamp'] - df['first_transaction_timestamp']).dt.total_seconds() / (60*60*24)

# Create a transactions-per-day feature
# Add a small number to avoid division by zero
df['transactions_per_day'] = df['total_transactions_sent'] / (df['account_active_days'] + 0.001)

# Select only the numerical features for the model
features_for_model = [
    'total_transactions_sent', 'total_ether_sent',
    'avg_ether_sent', 'account_active_days', 'transactions_per_day'
]

X = df[features_for_model].copy()

# Fill any potential missing values with the median
X.fillna(X.median(), inplace=True)

# Scale the features. This is crucial for anomaly detection algorithms.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Features have been engineered and scaled.")

# CELL 4: FEATURE ENGINEERING
import numpy as np
from sklearn.preprocessing import StandardScaler

# Create a feature for the account's active duration in days
df['account_active_days'] = (df['last_transaction_timestamp'] - df['first_transaction_timestamp']).dt.total_seconds() / (60*60*24)

# Create a transactions-per-day feature
# Add a small number to avoid division by zero
df['transactions_per_day'] = df['total_transactions_sent'] / (df['account_active_days'] + 0.001)

# Select only the numerical features for the model
features_for_model = [
    'total_transactions_sent', 'total_ether_sent',
    'avg_ether_sent', 'account_active_days', 'transactions_per_day'
]

X = df[features_for_model].copy()

# Fill any potential missing values with the median
X.fillna(X.median(), inplace=True)

# Scale the features. This is crucial for anomaly detection algorithms.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Features have been engineered and scaled.")

# CELL 5: BUILD AND TRAIN THE MODEL
from sklearn.ensemble import IsolationForest

# Initialize the Isolation Forest model.
iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)

print("Training the Isolation Forest model...")

# Train the model on your scaled data
iso_forest.fit(X_scaled)

# Get the predictions and ADD THE 'anomaly_flag' COLUMN
df['anomaly_flag'] = iso_forest.predict(X_scaled)

print("Model training and prediction complete.")

# CELL 6: REVIEW THE ANOMALIES

# Filter your DataFrame to see only the flagged anomalies
anomalies = df[df['anomaly_flag'] == -1].copy()

print(f"\nTotal accounts analyzed: {len(df)}")
print(f"Number of potential anomalies detected: {len(anomalies)}")

# Display the most significant anomalies, sorted by total ether sent
print("\n--- Sample of Flagged Anomalies ---")
print(anomalies.sort_values(by='total_ether_sent', ascending=False).head(15))